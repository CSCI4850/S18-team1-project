{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neocognitron Group\n",
    "## CSCI 4850/5850 Neural Networks - Dr. Phillips\n",
    "## proposal deadline: Thu. Mar. 1 @ 11:00pm\n",
    "#### Group Members:\n",
    "#### Buehler, Benjamin\n",
    "#### Coulter, Garrett\n",
    "#### Howell, Patrick\n",
    "#### Lutz, Kevin\n",
    "#### Mischke, Adam\n",
    "#### Sciullo, Eric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group is interested in working with creating an agent to solve tasks within the openai-gym toolkit with Tensorflow and Keras. To be specific, we're looking at different types of reinforcement algorithms like Asynchronous Advantage Actor-Critic (A3C) and deep Q-network (DQN) to win from their set of Atari 2600 games through the toolkit's environments. If we have the opportunity, we’ll test a Convolutional Neural Network (CNN) against the reinforcement learning to see which we can get to converge and maximize the fastest.\n",
    "\n",
    "The game will be the Breakout game, which was ported by Brad Stewart in 1978. Breakout is premised on staying alive and advancing through the many levels, as many games are. Our agent controls a paddle in the bottom part of the screen moves left or right while a ball moves in all directions. The agent may choose 3 actions: Move left, Move right, Don’t move. Based on where the ball hits on the paddle, the angle of the ball’s travel is divided into 6 angles during the first 7 bounces. After the 7th hit, the angles become more diverse and sharp. The angles then change to become even more sharp on the 12th bounce, which adds to an even more difficult ball hitting tactic. The paddle is required to hit the ball back into these destroyable blocks on the middle-top of the screen to increase its score, which is labeled in the top with three places as 000 or 001 for 0 and 1 points, respectively. The paddles lives are then the next number over, which represent how many times it can play a level without the game being reset. The agents only way of losing is by letting the ball travel below the paddle into the abyss. The agent can also hit the sides and top, which are solid layers of non-breakable bricks. The agent begins with only 5 lives and cannot gain lives through score. The last number at the top represents the level that the agent will be on, which is for tracking purposes only. Some of the blocks have interesting characteristics, such as the orange gradient ones, which double the speed of the ball. Some of the blocks are even able to be passed through without being at an exact angle. These are some of the aspects the agent will have to adapt to. A trick the agent might learn would be to trap the ball on the top part of the screen to break blocks without having to hit the ball. A way to encourage that behavior might be to reward more for how fast it can gain points. To speed up learning, we won’t have to display the screen or even show the game being played except through the agent and the game’s parameters.\n",
    "\n",
    "Our general algorithm will observe the game through RAM variables (128 bytes) or the screen through pixel data from the camera as input. The RBG image that is used is an array of shape (210, 160, 3) with the dimensions being (X, Y, RGB). We will then assign some sort of reward system for the game to increase or decrease a score depending on the actions taken in the previous observations and the environment variables, such as the ball location, whether it hit the ball, or how many bricks it destroys. Other good ideas would be to capture the X and Y positions of the ball to try to follow and align with it in order to not miss. A punishment would be for losing or moving too out of range of the balls range. The obvious big factor is to reward higher for destroying the bricks, which is the purpose of the game. By using reinforcement learning, we can reinforce the good behaviors each by accumulating the score each frame and from other previous frames in batches.\n",
    "\n",
    "Some tests and observations we’d like to run are how long and how fast it takes to train, what tricks it learns, where the agent gets stuck, how long it gets stuck, and how the agent learns to get out of those stuck patterns. If the agent gets stuck on particular parts, we may be able to plot and handle those problem areas with a methodical solution. For easy troubleshooting access, we have access to save states, reload states, and setting specific seeds. We may have to change or come up with many different network layers and units to encourage better results. These are all aspects that we will be able to graph through JupyterLab through loss and scores.\n",
    "\n",
    "Our goal is to succeed and be able to complete the game with minimal deaths. If we can get that as far as not dying and winning a couple of rounds, that would be great too. If all else fails, we can try switching to another game, although, this seems to be the easiest so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
