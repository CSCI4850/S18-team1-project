{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Toward a Deep Reinforcement Neural Network Capable of Playing Breakout}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\n",
    "\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Adam Mischke}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{2\\textsuperscript{nd} Patrick Howell}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{3\\textsuperscript{rd} Eric Sciullo}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{4\\textsuperscript{th} Kevin Lutz}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{5\\textsuperscript{th} Benjamin H. Buehler}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{6\\textsuperscript{th} Garrett Coulter}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "}\n",
    "\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "Since Google DeepMind conquered Atari video games have become a standard environment to measure a Deep Q reinforcement learning network agent. We implement a DQN agent to play the classic Atari game breakout. Unlike the similar the Google DeepMind team used, in which an agent was taught to play multiple Atari 2600 games from a few training games, we failed to achieve human-like performance. We believe this to be due to a combination of factors including the time which the network trained and the computational resources at our disposal.\n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "deep learning, Atari, Breakout, reinforcement learning, neural nets, DQN Agent, Q function, artificial intellegence, AI, python, Open AI Gym, Arcade Learning Environment\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "The goals of this project were to build and train a neural network capable of playing the Atari 2600 game, Breakout, and to observe the learning process of the network.  Videogames are a well known and appreciated topic in the computer science community and they are also an excellent platform for exploring the topic of training neural networks.  These are the most likely reasons for the existence of the wealth of knowledge and tools available on the topic of teaching neural networks to play videogames in partially obstructed environments - i.e., the game learns from the same data a human player would have while playing.  Training networks to play in these partially obstructed environments affords the opportunity to watch a network learn to do a task with which the creators are probably intimately familiar.  Familiarity with the task makes the learning process significantly less abstract, especially when compared against something like learning to sort pictures or recognize language. The videogame playing network also makes it easier to explain the process to new or even non computer scientists, because the typical language of reward and punishment, or any of the other human related motivational terms typically used to explain neural networks start to make more sense.  For example, describing a network, playing a shooter, as avoiding the suffering of death, or negative reward values, by running away from enemies, until it learns it has a weapon it can use.  Because we understand the process of learning how to play a game, we can better understand the process a neural network goes through when learning to play a game."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "To develop this neural network we used the OpenAI Gym toolkit for integrating our reinforcement learning with Breakout. We used OpenAI Gym because it has a common interface for Atari games.\\cite{AiGym} Reinforcement learning takes an agent that is a part of a developed environment and is given a specific action as an intake and receives a reward and a notification/observation from the environment based on the action it takes.\\cite{ReLer} The Reinforcement learning algorithm's goal is to increase the reward to the agent as it interacts with the environment it is placed in.\\cite{ReLer}\n",
    "\n",
    "The learning the agent goes through can be broken down into sections or episodes.\\cite{ReLer} This is known as episodic reinforcement learning. With this the agents is placed at an initial state and proceeds until it reaches a terminal state or the end.\\cite{ReLer}\n",
    "\n",
    "In our case the environment our agent was placed in was the Atari 2600 game Breakout. Breakout is a game that was inspired by Pong. In this game there is a layer of bricks on the top portion of the screen. The agent release the ball allowing it to bounce accordingly from the paddle to the walls and hitting a brick. When a brick is hit it is destroyed. The agent loses when the ball misses the paddle when returning from hitting a brick and lands at the bottom of the screen.\n",
    "\n",
    "In our case when the agent loses a life/the ball hits the bottom of the screen it is reset and punished, while it is rewarded when a brick is destroyed. This ties back to the reinforcement learning we set our agent on. The agent is rewarded with every destroyed brick, negatively rewarded (punished) with every loss, and the episodes are each game cycle."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\\subsection*{Environment}\n",
    "We are using 'BreakoutDeterministic-v4' in Open AI Gym. \\cite{AiGym} The Atari environments in Open AI Gym use the Arcade Learning Environment. \\cite{AiGym}\\cite{ALE} Open AI Gym and the Arcade Learning Environment provide a useful way of accessing game data as input for a neural network. \\cite{AiGym}\\cite{ALE} We accessed only pixel data from this environment and not internal game states.\n",
    "\n",
    "\\subsection*{Input Processing}\n",
    "We downconverted the images to 84x84 grayscale from a 210x160 rgb image in a similar manner to what was done by Mnih et al. for their DQN network. \\cite{HumLevCon} We threw away all but every 4th frame in process called frame skipping commonly used to decrease training time in video game playing agents. \\cite{HumLevCon} \\cite{PlayAtariDeep} We discretize the input by packaging four frames into the neural network's input data structure as discussed by Max Lapin in his Medium Article Speeding up DQN on PyTorch: how to solve Pong in 30 minutes. \\cite{SpeedDQN}\n",
    "\n",
    "\\subsection*{Topology of the Net and Loss Functions}\n",
    "Our neural net uses three convolutional layers to process visual features in the input pixels. They have a kernel of 8x8, 4x4, and 3x3 respectively. The net is then flattened out and two dense layers follow with 512 neurons and 3 neurons respectively. ReLU is used for all the layers except for the last which is linear. We use the huber loss function.\n",
    "\n",
    "\\subsection*{Reinforcement Learning Algorithm}\n",
    "    We used a reinforcement learning approach to create our agent. This involved using DQN networks to approximate a $ Q^* $ function in a similar manner to Mnih et al.. \\cite{HumLevCon} Additionally, we employed an $\\epsilon$ greedy exploration approach. Our agent uses an offline learning method with replay memory in order to prevent catastrophic forgetting.\n",
    "    \n",
    "\\subsection*{Training}\n",
    "    We were able to train the agent for almost 82 hours (294661 seconds). This was well after $ \\epsilon $ reached its minimum during training. Because as $ \\epsilon $ approaches $ 0 $ the probability of choosing the action with the highest $ Q $ value increases, exploration had diminished by this point.\\cite{ReLer}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{figure}[htbp]\n",
    "    \\centerline{\\includegraphics[width=\\linewidth]{ep_vs_scr.png}}\n",
    "    \\label{fig1}\n",
    "    \\caption{Training Data}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "After training for over $ 3 $ days and $ 1.75 $ million frames the network was still unable to do much more than track the ball from the start position. Observing the data from different training sessions, one may note the lack of consistency. This is due to the $ \\epsilon $ value starting at $ 1 $, or more exploration, and slowly decreasing to $ 0.1 $, or less exploration, over the course of the first million frames. This can be seen in Figure 1 (\\ref{fig1}) in how there is a great degree of variation, by design, from the beginning of the training session up to around 4500 episodes. The first episode to go over a million frames is episode 4289. One may observe that shortly after that the core of the reward values tightens up, and while the core, or most frequent values of the graph decrease, they become more consistent. In the last third of the graph the bulk variation goes from between $ 1 $ and $ 7 $ for episode $ 3000 $ to $ 4500 $, to $ 1 $ and $ 4 $ after.  This shows that the network is acting in a more consistent manner.\n",
    "    In Human-level Control through Deep Reinforcement Learning, the network was trained for 38 days worth of playing time and $ 50 $ million frames.\\cite{HumLevCon} While that network was being designed to learn how to play any Atari 2600 game, given the relatively large amount of variance caused by the speed of the ball based on what surfaces it contacts and the variation in the angle of the bounce off the paddle based on the position of the paddle the ball strikes, $ 1.75 $ million frames is a fraction of the necessary training time required to train a network to play at a human level. Even when considering that our network is only training to play one game."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "While expected, the results of training the neural network were quite interesting to view first-hand.  We understood that the rewards would be much more randomly distributed for the first million frames, but when oneâ€™s prior experiences include mostly training lasting minutes, one tends to expect results when the network is given a training period spanning multiple days.  The amount of time our network trained to gain what little proficiency it had, draws attention to how incredible it is that our minds are able to take our previous experiences and the output of the game and solve the problem so quickly.  We as humans are able to draw on our previous experiences and even to apply them holistically.  We do these studies to better understand understand not only how teach a network, but also to better understand how humans try to solve problems at a fundamental level.  Having the technology to teach a neural network to play a game at a superhuman level, with enough training, is the first step in teaching the network to apply what it knows about one game to playing another game, or eventually, teaching it to apply skills that are only tangentially related to new problems.  However, within the scope of this project, what might be an interesting direction would be growing the capabilities of the network to eventually include games beyond the Atari 2600.  Obviously, changes would need to be made to the way input is received and processed, but what if the network could recognize patterns in similar game design and apply it to more modern games?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{References} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
