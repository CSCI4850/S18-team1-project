{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Toward a Deep Reinforcement Neural Network Capable of Playing Breakout}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\n",
    "\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Adam Mischke}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{2\\textsuperscript{nd} Patrick Howell}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{3\\textsuperscript{rd} Eric Sciullo}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{4\\textsuperscript{th} Kevin Lutz}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{5\\textsuperscript{th} Benjamin H. Buehler}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "\\and\n",
    "\\IEEEauthorblockN{6\\textsuperscript{th} Garrett Coulter}\n",
    "\\IEEEauthorblockA{\\textit{Computer Science Undergraduate} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN, USA \\\\}\n",
    "}\n",
    "\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "Since Google DeepMind conquered Atari video games have become a standard environment to measure a Deep Q reinforcement learning network agent. We implement a DQN agent to play the classic Atari game breakout. Unlike the similar the Google DeepMind team used, in which an agent was taught to play multiple Atari 2600 games from a few training games, we failed to achieve human-like performance. We believe this to be due to a combination of factors including the time which the network trained and the computational resources at our disposal.\n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "deep learning, Atari, Breakout, reinforcement learning, neural nets, DQN Agent, Q function, artificial intellegence, AI, python, Open AI Gym, Arcade Learning Environment\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "The goals of this project were to build and train a neural network capable of playing the Atari 2600 game, Breakout, and to observe the learning process of the network. Videogames are a well known and appreciated topic in the computer science community and they are also an excellent platform for exploring the topic of training neural networks.\\cite{PlayAtariDeep}\\cite{SimplePlayAtariDeep}  These are the most likely reasons for the existence of the wealth of knowledge and tools available on the topic of teaching neural networks to play videogames in partially obstructed environments - i.e., the game learns from the same data a human player would have while playing.  Training networks to play in these partially obstructed environments affords the opportunity to watch a network learn to do a task with which the creators are probably intimately familiar.  Familiarity with the task makes the learning process significantly less abstract, especially when compared against something like learning to sort pictures or recognize language. The videogame playing network also makes it easier to explain the process to new or even non computer scientists, because the typical language of reward and punishment, or any of the other human related motivational terms typically used to explain neural networks start to make more sense.  For example, describing a network, playing a shooter, as avoiding the suffering of death, or negative reward values, by running away from enemies, until it learns it has a weapon it can use.  Because we understand the process of learning how to play a game, we can better understand the process a neural network goes through when learning to play a game."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "To develop this neural network we used the OpenAI Gym toolkit for integrating our reinforcement learning with Breakout. We used OpenAI Gym because it has a common interface for Atari games.\\cite{AiGym} Reinforcement learning takes a game state and predicts which action is optimal given previous experience, trying to optimize a reward function.\\cite{ReLer} The Reinforcement learning algorithm's goal is to increase the reward to the agent as it interacts with the environment it is placed in.\\cite{ReLer}\n",
    "\n",
    "Because previous experience determines the expected reward and unexplored game states are assumed to lead to a reward of zero, an agent could easily get stuck in a loop if it were to pursue the maximum expected reward for every state. \\cite{ReLer} The agent must have some mechanism to break out of this loop. \\cite{ReLer} This problem has been called the multi-armed bandit problem. \\cite{ReLer} Supposing an agent is at a casino in front of a row of slot machines with $ n $ quarters with the probability of payout for each machine unknown, what should a rational agent do?\\cite{ReLer} An agent maximizing the expected reward would pick a random slot machine until one of the machines paid out and would then play that machine until they lost more money than their initial first attempt.\\cite{ReLer} Similarly, an agent picking random slot machines could be picking the 'duds' more often than necessary.\\cite{ReLer} Our solution to this problem, commonly used by implementers of reinforcement learning algorithms, was an $ \\epsilon $-greedy approach.\\cite{HumLevCon}\n",
    "\n",
    "The learning the agent goes through can be broken down into sections or episodes.\\cite{ReLer} This is known as episodic reinforcement learning. With this the agents is placed at an initial state and proceeds until it reaches a terminal state or the end.\\cite{ReLer}\n",
    "\n",
    "Both our group and Google DeepMind used a form of called DQN reinforcement learning.\\cite{HumLevCon} A $ Q^{\\pi} $ function, also called an action-value function for policy $ \\pi $, maps from the cross product of the set of all possible states in the problem and the set of possible actions at that state to the expected reward when the policy (rule for picking actions) $ \\pi $ is followed.\\cite{ReLer} $ Q^* $, also called the optimal action-value function has the same domain and codomain but returns the expected reward if the agent followes the optimal policy throughout the rest of the episode.\\cite{ReLer} The function of the neural network in DQN reinforcement learning is to use its experience to approximate this optimal action-value function.\\cite{HumLevCon} Once an approximation is found (assuming the number of possible actions at any given time is low) picking the correct action for the agent is trivial because all that is needed is to pick the action $ a' $ for which the $ Q^* $ value is highest for every state $ s $.\\cite{HumLevCon}\\cite{ReLer}\n",
    "\n",
    "In our case the environment our agent was placed in was the Atari 2600 game Breakout. Breakout is a game that was inspired by Pong. In this game there are several layers of bricks on the top portion of the screen and a moveable paddle on the bottom of the screen. The agent releases the ball allowing it to bounce accordingly from the paddle to the walls and hitting a brick. When a brick is hit it is destroyed and gives the agent points. The agent loses a life when the ball misses the paddle and lands at the bottom of the screen. When the agent loses all lives the game ends.\n",
    "\n",
    "In our case when the agent loses a life/the ball hits the bottom of the screen it is reset while it is rewarded only when a brick is destroyed. There is no punishment for this cycle, as our intentions were for it to learn to keep the ball in play. The longer that it did so, meant that the ball would learn to continue to do so, only being rewarded for ‘good’ episodes. The Open AI Gym wrapper was observed to only return a reward of 1 when a brick was destroyed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\\subsection{Environment}\n",
    "We used the ‘BreakoutNoFrameskip-v4’ in Open AI Gym, and implemented our own frameskipping, repeating the same action 4 times in a row for those skipped frames as shown in Figure 2 (\\ref{fig2}). We then also tryed another environment, ‘BreakoutDeterministic-v4’ that does the same, except implemented by Open AI Gym. \\cite{AiGym} The Atari environments in Open AI Gym use the Arcade Learning Environment. \\cite{AiGym}\\cite{ALE} Open AI Gym and the Arcade Learning Environment provide a useful way of accessing game data as input for a neural network. \\cite{AiGym}\\cite{ALE} We accessed only pixel data from this environment and not internal game states.\n",
    "\n",
    "\\subsection{Input Processing}\n",
    "We downsampled the images to 84x84 then converted to grayscale (0-255) from a 210x160 rgb image in a similar manner to what was done by Mnih et al. for their DQN network. \\cite{HumLevCon} We would train on normalized (0.0-1.0) float32 image data, but would store the images in as uint8 to reduce memory footprint, as 1,000,000 size numpy arrays are huge. With discrete framing, meaning 2 seperate 4 frame windows of array size (1,000,000, 84, 84, 4) for current and next frame, the memory taken up would be around 65Gb. As for a more efficient frame sliding technique, where we store one (1,000,000, 84, 84, 5) array with the first four channels being current frame and the next four being next frame, the memory taken was around 45Gb. Additionaly, with frame skipping, which is a common way to decrease training time in video game playing agents. \\cite{HumLevCon} \\cite{PlayAtariDeep} We discretize the input by packaging four frames into the neural network’s input data structure as discussed by Max Lapin in his Medium Article Speeding up DQN on PyTorch: how to solve Pong in 30 minutes. \\cite{SpeedDQN}\n",
    "\n",
    "\\subsection{Topology of the Net and Loss Functions}\n",
    "Our neural net uses three convolutional layers to process visual features in the input pixels. They have a kernel of 8x8, 4x4, and 3x3 respectively. The net is then flattened out and two dense layers follow with 512 neurons and 3 neurons respectively. ReLU is used for all the layers except for the last which is linear for the 3 output buttons. The three most commonly used loss functions were implemented: the Logarithm of the Hyperbolic Cosine, Mean Squared Error, and Huber loss.\n",
    "\n",
    "\\subsection{Reinforcement Learning Algorithm}\n",
    "   We used a reinforcement learning approach to create our agent. This involved using DQN networks to approximate a $ Q^* $ function in a similar manner to Mnih et al.. \\cite{HumLevCon} Additionally, we employed an $\\epsilon$ greedy exploration approach. Our agent uses an offline learning method with replay memory in order to prevent catastrophic forgetting.\n",
    "\n",
    "\\subsection*{Training}\n",
    "   We were able to train the agent for almost 82 hours (294661 seconds). This was well after $ \\epsilon $ reached its minimum during training. Because as $ \\epsilon $ approaches $ 0 $ the probability of choosing the action with the highest $ Q $ value increases, exploration had diminished by this point.\\cite{ReLer} We implemented a linear annealing $ \\epsilon $ that started at 1.0 and ended at 0.1 for 1 millions frames as per the Deepmind paper. \\cite{HumLevCon}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{figure}[htbp]\n",
    "    \\centerline{\\includegraphics[width=\\linewidth]{ep_vs_scr.png}}\n",
    "    \\label{fig1}\n",
    "    \\caption{Training Data}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{figure}[htbp]\n",
    "    \\centerline{\\includegraphics[width=\\linewidth]{fig2.png}}\n",
    "    \\label{fig1}\n",
    "    \\caption{Custom frame skipping method.}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "After training for over $ 3 $ days and $ 1.75 $ million frames the network was still unable to do much more than track the ball from the start position. Observing the data from different training sessions, one may note the lack of consistency. This is due to the $ \\epsilon $ value starting at $ 1 $, or more exploration, and slowly decreasing to $ 0.1 $, or less exploration, over the course of the first million frames. This can be seen in Figure 1 (\\ref{fig1}) in how there is a great degree of variation, by design, from the beginning of the training session up to around 4500 episodes. The first episode to go over a million frames is episode 4289. One may observe that shortly after that the core of the reward values tightens up, and while the core, or most frequent values of the graph decrease, they become more consistent. In the last third of the graph the bulk variation goes from between $ 1 $ and $ 7 $ for episode $ 3000 $ to $ 4500 $, to $ 1 $ and $ 4 $ after.  This shows that the network is acting in a more consistent manner.\n",
    "    In Human-level Control through Deep Reinforcement Learning, the network was trained for 38 days worth of playing time and $ 50 $ million frames.\\cite{HumLevCon} While that network was being designed to learn how to play any Atari 2600 game, given the relatively large amount of variance caused by the speed of the ball based on what surfaces it contacts and the variation in the angle of the bounce off the paddle based on the position of the paddle the ball strikes, $ 1.75 $ million frames is a fraction of the necessary training time required to train a network to play at a human level. Even when considering that our network is only training to play one game."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "While expected, the results of training the neural network were quite interesting to view first-hand.  We understood that the rewards would be much more randomly distributed for the first million frames, but when one’s prior experiences include mostly training lasting minutes, one tends to expect results when the network is given a training period spanning multiple days.  The amount of time our network trained to gain what little proficiency it had, draws attention to how incredible it is that our minds are able to take our previous experiences and the output of the game and solve the problem so quickly.  We as humans are able to draw on our previous experiences and even to apply them holistically.  We do these studies to better understand understand not only how teach a network, but also to better understand how humans try to solve problems at a fundamental level.  Having the technology to teach a neural network to play a game at a superhuman level, with enough training, is the first step in teaching the network to apply what it knows about one game to playing another game, or eventually, teaching it to apply skills that are only tangentially related to new problems.  However, within the scope of this project, what might be an interesting direction would be growing the capabilities of the network to eventually include games beyond the Atari 2600.  Obviously, changes would need to be made to the way input is received and processed, but what if the network could recognize patterns in similar game design and apply it to more modern games?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{References} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
